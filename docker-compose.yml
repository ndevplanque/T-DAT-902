name: t-dat-902

services:
  backend:
    build: ./backend
    ports:
      - "5001:5001"
    volumes:
      - ./backend:/app
    environment:
      - FLASK_APP=api/main.py
      - FLASK_ENV=development # For hot-reloading
      - FLASK_DEBUG=1 # For hot-reloading
    networks:
      - t-dat-902-network
    depends_on:
      - postgres

  frontend:
    build: ./frontend
    ports:
      - "8501:8501"
    volumes:
      - ./frontend:/app
    environment:
      - STREAMLIT_SERVER_HEADLESS=true
      - STREAMLIT_DEVELOPMENT=true
      - STREAMLIT_SERVER_RUN_ON_SAVE=true
    depends_on:
      - backend
    networks:
      - t-dat-902-network

  # PostgreSQL - utilise l'émulation automatique
  postgres:
    image: postgis/postgis:14-3.3
    platform: linux/amd64
    container_name: postgres
    restart: always
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=gis_db
      - POSTGRES_INITDB_ARGS=--wal-segsize=1
      - POSTGRES_INITDB_WALDIR=/var/lib/postgresql/pg_wal
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - postgres-wal:/var/lib/postgresql/pg_wal
      - ./sql/create_polygons_tables.sql:/docker-entrypoint-initdb.d/create_polygons_tables.sql
    networks:
      - t-dat-902-network

  # Hadoop avec platform spécifique
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    platform: linux/amd64
    container_name: namenode
    restart: always
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./geo_importer/geojson_files:/geojson_files
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      - t-dat-902-network

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    platform: linux/amd64
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
      - ./geo_importer/geojson_files:/geojson_files
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    depends_on:
      - namenode
    networks:
      - t-dat-902-network

  spark-master:
    image: bitnami/spark:3.4.1
    platform: linux/amd64
    container_name: spark-master
    restart: always
    environment:
      - SPARK_MODE=master
      - JAVA_OPTS=-Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./geo_importer/jars:/opt/bitnami/spark/custom-jars
    networks:
      - t-dat-902-network

  spark-worker:
    build:
      context: ./geo_importer
      dockerfile: Dockerfile.spark-worker
    platform: linux/amd64
    container_name: spark-worker
    restart: always
    environment:
      - PYSPARK_PYTHON=/usr/bin/python3.9
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - JAVA_HOME=/opt/bitnami/java
      - SPARK_CLASSPATH=/opt/bitnami/spark/custom-jars/*
      - JAVA_OPTS=-Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
    depends_on:
      - spark-master
    volumes:
      - ./geo_importer/jars:/opt/bitnami/spark/custom-jars
    networks:
      - t-dat-902-network

  spark-worker-2:
    build:
      context: ./geo_importer
      dockerfile: Dockerfile.spark-worker-2
    platform: linux/amd64
    container_name: spark-worker-2
    restart: always
    environment:
      - PYSPARK_PYTHON=/usr/bin/python3.9
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - JAVA_HOME=/opt/bitnami/java
      - SPARK_CLASSPATH=/opt/bitnami/spark/custom-jars/*
      - JAVA_OPTS=-Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
    depends_on:
      - spark-master
    volumes:
      - ./geo_importer/jars:/opt/bitnami/spark/custom-jars
    networks:
      - t-dat-902-network

  # HDFS Loader
  hdfs-loader:
    image: bde2020/hadoop-base:2.0.0-hadoop3.2.1-java8
    platform: linux/amd64
    container_name: hdfs-loader
    depends_on:
      - namenode
      - datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
    volumes:
      - ./geo_importer/geojson_files:/geojson_files
      - ./geo_importer/hdfs-loader.sh:/hdfs-loader.sh
    command: bash /hdfs-loader.sh
    networks:
      - t-dat-902-network
    restart: on-failure

  # Geo Data Importer
  geo-data-importer:
    build:
      context: ./geo_importer
      dockerfile: Dockerfile
    container_name: geo-data-importer
    restart: on-failure
    depends_on:
      - postgres
      - namenode
      - datanode
      - spark-master
      - spark-worker
      - hdfs-loader
    environment:
      - POSTGRES_DB=gis_db
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - SPARK_APP_NAME=GeoJSON_Import
      - SPARK_MASTER=spark://spark-master:7077
      - HDFS_CITIES_PATH=hdfs://namenode:9000/data/communes-1000m.geojson
      - HDFS_DEPARTMENTS_PATH=hdfs://namenode:9000/data/departements-1000m.geojson
      - HDFS_REGIONS_PATH=hdfs://namenode:9000/data/regions-1000m.geojson
      - SPARK_JARS=/app/jars/postgresql-42.7.4.jar
      - JAVA_OPTS=-Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
      - PYSPARK_PYTHON=/usr/bin/python3.9
      - PYSPARK_DRIVER_PYTHON=/usr/bin/python3.9
    volumes:
      - ./geo_importer:/app
    networks:
      - t-dat-902-network

  # MongoDB pour stocker les avis scraped
  mongodb:
    image: mongo:latest
    container_name: mongodb
    restart: always
    ports:
      - "27017:27017"
    environment:
      - MONGO_INITDB_ROOT_USERNAME=root
      - MONGO_INITDB_ROOT_PASSWORD=rootpassword
    volumes:
      - mongodb_data:/data/db
      - ./avis-scraper/mongo-dump:/dump  # Monter le répertoire avec les dumps MongoDB
    networks:
      - t-dat-902-network
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/admin -u root -p rootpassword --quiet
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 40s

  # Service de restauration MongoDB qui s'exécute conditionnellement
  mongodb-restore:
    image: mongo:latest
    container_name: mongodb-restore
    restart: "no"
    depends_on:
      mongodb:
        condition: service_healthy
    volumes:
      - ./avis-scraper/mongo-dump:/dump  # Monter le répertoire avec les dumps MongoDB
    command: |
      bash -c '
      if [ "$USE_MONGODB_DUMP" = "true" ]; then
        echo "Restauration du dump MongoDB..."
        if [ -d "/dump/villes_france" ]; then
          mongorestore --host mongodb --port 27017 --username root --password rootpassword --authenticationDatabase admin --db villes_france /dump/villes_france
          echo "Restauration terminée!"
        else
          echo "ERREUR: Le dump MongoDB n existe pas à l emplacement /dump/villes_france"
          echo "Assurez-vous que le dump existe dans le répertoire avis-scraper/mongo-dump/villes_france/"
          exit 1
        fi
      else
        echo "Restauration ignorée - le scraping sera utilisé à la place."
      fi
      '
    environment:
      - USE_MONGODB_DUMP=${USE_MONGODB_DUMP:-false}
    networks:
      - t-dat-902-network

  # Service de scraping des avis - conditionnel en fonction de USE_SCRAPER
  avis-scraper:
    build:
      context: ./avis-scraper
      dockerfile: Dockerfile
    container_name: avis-scraper
    restart: on-failure
    environment:
      - MONGO_URI=mongodb://root:rootpassword@mongodb:27017/
      - MONGO_DB=villes_france
      - MAX_VILLES=${MAX_VILLES:-0}  # Limiter pour le test, mettre à 0 pour tout scraper
      - MAX_WORKERS=${MAX_WORKERS:-8}  # Nombre de threads parallèles
      - TEST_MODE=${TEST_MODE:-false}  # Activer/désactiver le mode test
      - UPDATE_MODE=${UPDATE_MODE:-false}  # Activer/désactiver le mode mise à jour
      - ENABLE_SCRAPER=${ENABLE_SCRAPER:-true}  # Nouvelle variable pour activer/désactiver le scraper
    depends_on:
      mongodb:
        condition: service_healthy
      mongodb-restore:
        condition: service_completed_successfully
    volumes:
      - ./avis-scraper:/app
      - ./logs:/app/logs
    networks:
      - t-dat-902-network
    # Utiliser une condition d'entrypoint pour exécuter ou non le scraper
    entrypoint: |
      sh -c '
      if [ "$ENABLE_SCRAPER" = "true" ]; then
        echo "Démarrage du scraper..."
        python /app/scraper.py
      else
        echo "Scraper désactivé - utilisation des données du dump."
        exit 0
      fi
      '

  avis-data-validator:
    build:
      context: ./avis-scraper/validation
      dockerfile: Dockerfile
    container_name: avis-data-validator
    volumes:
      - ./avis-scraper/validation:/app
      - ./logs:/app/logs
    environment:
      - MONGO_URI=mongodb://root:rootpassword@mongodb:27017/
      - MONGO_DB=villes_france
    networks:
      - t-dat-902-network
    depends_on:
      - mongodb
      - avis-scraper

  # Service pour executer le traitement des avis des villes pour le nuage de mots et les sentiments simples
  avis-processor-submitter:
    build:
      context: ./avis-processor
      dockerfile: Dockerfile
    container_name: avis-processor-submitter
    restart: on-failure
    environment:
      - MONGO_URI=mongodb://root:rootpassword@mongodb:27017/
      - MONGO_DB=villes_france
      - WAIT_FOR_SCRAPER=${ENABLE_SCRAPER:-true}  # Ne pas attendre le scraper s'il est désactivé
    depends_on:
      - mongodb
      - avis-scraper
    volumes:
      - ./avis-processor:/app
      - ./logs:/app/logs
    networks:
      - t-dat-902-network

  # Service de validation des données traitées pour le nuage de mots
  avis-processor-validator:
    build:
      context: ./avis-processor/validation
      dockerfile: Dockerfile
    container_name: avis-processor-validator
    volumes:
      - ./avis-processor/validation:/app
      - ./logs:/app/logs
      - ./avis-processor/validation/results:/app/results
    environment:
      - MONGO_URI=mongodb://root:rootpassword@mongodb:27017/
      - MONGO_DB=villes_france
      - OUTPUT_DIR=/app/results
    networks:
      - t-dat-902-network
    depends_on:
      - mongodb
      - avis-processor-submitter

  # Service pour les agrégations par département et région
  data-aggregator:
    build:
      context: ./data-aggregator
      dockerfile: Dockerfile
    container_name: data-aggregator
    restart: on-failure
    environment:
      - MONGO_URI=mongodb://root:rootpassword@mongodb:27017/
      - MONGO_DB=villes_france
      - POSTGRES_DB=gis_db
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
    depends_on:
      - mongodb
      - postgres
      - avis-scraper
      - avis-processor-submitter
      - avis-processor-validator
      - geo-data-importer
    volumes:
      - ./data-aggregator:/app
      - ./logs:/app/logs
    networks:
      - t-dat-902-network

  # Service de validation des agrégations
  data-aggregator-validator:
    build:
      context: ./data-aggregator/validation
      dockerfile: Dockerfile.validator
    container_name: data-aggregator-validator
    restart: "no"
    environment:
      - MONGO_URI=mongodb://root:rootpassword@mongodb:27017/
      - MONGO_DB=villes_france
      - POSTGRES_DB=gis_db
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - OUTPUT_DIR=/app/results
    depends_on:
      - mongodb
      - postgres
      - data-aggregator
    volumes:
      - ./data-aggregator/validation:/app
      - ./logs:/app/logs
      - ./data-aggregator/validation/results:/app/results
    networks:
      - t-dat-902-network

networks:
  t-dat-902-network:
    driver: bridge

volumes:
  postgres-data:
  postgres-wal:
  hadoop_namenode:
  hadoop_datanode:
  mongodb_data: